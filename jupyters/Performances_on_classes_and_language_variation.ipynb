{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_OUT_PATH = 'cherry_picked/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = ['adapter_16', 'adapter_2', 'bert_ft_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.ini']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = configparser.ConfigParser()\n",
    "parser.read('data.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "base_results_path = 'official_results/predictions/'\n",
    "onoe_base_results_path = 'results/avgs_stds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(suffixes, base_path):\n",
    "    files = []\n",
    "    for file in os.listdir(base_path):\n",
    "        for suffix in suffixes:\n",
    "            if re.search(suffix, file):\n",
    "                files.append(base_path + file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "def trimmed_stats(x, sampled = True):\n",
    "    x_sorted = np.sort(x)[1:-1]\n",
    "    return x_sorted.mean(), x_sorted.std(ddof = 1 if sampled else 0)\n",
    "\n",
    "def extract_f1s_from_onoe(f): \n",
    "    f1s = {}\n",
    "    with open(f, 'r') as inp:\n",
    "        f_lines = inp.readlines()\n",
    "        for l in f_lines[1:]:\n",
    "            elems = l.split('\\t')\n",
    "            typ = elems[0].strip()\n",
    "            if typ not in test_types:\n",
    "                f1 = float(elems[1])\n",
    "                f1s[typ] = f1\n",
    "                test_types.append(typ)\n",
    "    return f1s\n",
    "\n",
    "def extract_f1s_from_file(f): \n",
    "    regex = r'_[0-9]+'\n",
    "    test_types = [] \n",
    "    f1s = defaultdict(list)\n",
    "    update_i = {}\n",
    "    with open(f, 'r') as inp:\n",
    "        f_lines = inp.readlines()\n",
    "        delimiter = f_lines[0]\n",
    "        model_i = 0\n",
    "        for l in f_lines[1:]:\n",
    "            if l != delimiter:\n",
    "                elems = l.split('\\t')\n",
    "                typ = re.sub(regex, '', elems[0])\n",
    "                if typ in update_i and update_i[typ] != model_i:\n",
    "                    f1 = float(elems[3])\n",
    "                    f1s[typ].append(f1)\n",
    "                    update_i[typ] = model_i\n",
    "                elif typ not in update_i:\n",
    "                    update_i[typ] = 0\n",
    "                if typ not in test_types:\n",
    "                    test_types.append(typ)\n",
    "            else:\n",
    "                model_i += 1\n",
    "    f1s = {k:trimmed_stats(v)[0] for k, v in f1s.items()}\n",
    "    return f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_f1_per_label(f1s, label):\n",
    "    return {k: v[label] for k, v in f1s.items() if label in v}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read_dataset: 100%|██████████| 251039/251039 [00:04<00:00, 55418.72it/s]\n",
      "reading dataset and generate sentences...: 100%|██████████| 251039/251039 [00:00<00:00, 456783.48it/s]\n",
      "get classes...: 100%|██████████| 251039/251039 [00:00<00:00, 595082.65it/s]\n",
      "get_class_sentences...: 100%|██████████| 89/89 [00:00<00:00, 763.99it/s]\n",
      "read_dataset: 100%|██████████| 8963/8963 [00:00<00:00, 35752.30it/s]\n",
      "reading dataset and generate sentences...: 100%|██████████| 8963/8963 [00:00<00:00, 448187.83it/s]\n",
      "get classes...: 100%|██████████| 8963/8963 [00:00<00:00, 673924.79it/s]\n",
      "get_class_sentences...: 100%|██████████| 67/67 [00:00<00:00, 15057.51it/s]\n",
      "read_dataset: 100%|██████████| 528/528 [00:00<00:00, 54226.07it/s]\n",
      "reading dataset and generate sentences...: 100%|██████████| 528/528 [00:00<00:00, 639501.16it/s]\n",
      "get classes...: 100%|██████████| 528/528 [00:00<00:00, 687334.73it/s]\n",
      "get_class_sentences...: 100%|██████████| 35/35 [00:00<00:00, 26546.23it/s]\n",
      "read_dataset: 100%|██████████| 11483/11483 [00:00<00:00, 58096.05it/s]\n",
      "reading dataset and generate sentences...: 100%|██████████| 11483/11483 [00:00<00:00, 461895.15it/s]\n",
      "get classes...: 100%|██████████| 11483/11483 [00:00<00:00, 672229.02it/s]\n",
      "get_class_sentences...: 100%|██████████| 29/29 [00:00<00:00, 9109.86it/s]\n",
      "read_dataset: 100%|██████████| 1532/1532 [00:00<00:00, 44238.41it/s]\n",
      "reading dataset and generate sentences...: 100%|██████████| 1532/1532 [00:00<00:00, 601880.27it/s]\n",
      "get classes...: 100%|██████████| 1532/1532 [00:00<00:00, 664289.64it/s]\n",
      "get_class_sentences...: 100%|██████████| 69/69 [00:00<00:00, 24561.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from cherry_picker import pick, write\n",
    "dataset = 'OntoNotes'\n",
    "\n",
    "picker = pick(dataset_name = dataset, n = 20, data=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = '/other/product'\n",
    "\n",
    "picked = picker(label)\n",
    "out_directory = BASE_OUT_PATH + dataset\n",
    "\n",
    "write(out_directory, label, picked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labels performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = [r'tested_on_onto_filtered_with_[a-z]+_test.txt',\n",
    "            r'tested_on_onto_test.txt']\n",
    "\n",
    "#suffixes = [r'tested_on_bbn_filtered_with_[a-z]+_test.txt',\n",
    "#            r'tested_on_bbn_test.txt']\n",
    "\n",
    "#suffixes = [r'tested_on_choi_filtered_with_[a-z]+_test.txt',\n",
    "#            r'tested_on_choi_test.txt']\n",
    "\n",
    "#suffixes = [r'tested_on_figer_filtered_with_[a-z]+_test.txt',\n",
    "#            r'tested_on_figer_test.txt']\n",
    "\n",
    "#onoes_suffixes = ['OntoNotes_preds.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = extract_files(suffixes, base_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = {}\n",
    "\n",
    "for f in files:\n",
    "    train_dataset = f.split('trained_on')[1].split('_')[1]\n",
    "    model_name = f.split('_trained_on')[0].split('/')[-1]\n",
    "    f1s[model_name + '_trained_on_' + train_dataset] = extract_f1s_from_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adapter_16_trained_on_bbn': 0.215525,\n",
       " 'adapter_16_trained_on_choi': 0.618675,\n",
       " 'adapter_16_trained_on_figer': 0.4262,\n",
       " 'adapter_16_trained_on_onto': 0.0,\n",
       " 'adapter_2_trained_on_bbn': 0.21702500000000002,\n",
       " 'adapter_2_trained_on_choi': 0.5823,\n",
       " 'adapter_2_trained_on_figer': 0.36430000000000007,\n",
       " 'adapter_2_trained_on_onto': 0.0,\n",
       " 'bert_ft_0_trained_on_bbn': 0.0874,\n",
       " 'bert_ft_0_trained_on_choi': 0.432875,\n",
       " 'bert_ft_0_trained_on_figer': 0.15425,\n",
       " 'bert_ft_0_trained_on_onto': 0.10505,\n",
       " 'bert_ft_2_trained_on_bbn': 0.144025,\n",
       " 'bert_ft_2_trained_on_choi': 0.5556500000000001,\n",
       " 'bert_ft_2_trained_on_figer': 0.29822499999999996,\n",
       " 'bert_ft_2_trained_on_onto': 0.0}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_f1_per_label(f1s, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
